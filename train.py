import argparse
parser = argparse.ArgumentParser(
		description='Train a language model on text file.',
		formatter_class=argparse.ArgumentDefaultsHelpFormatter
	)
parser.add_argument(
		'data_file',
		type = argparse.FileType('r'),
		help = "Text file for training. Each new line will be treated as a new sequence"
	)
parser.add_argument(
		'vocab_file',
		type = argparse.FileType('r'),
		help = "Vocabulary file generated by vocab.py."
	)
parser.add_argument(
		'output_file',
		type = argparse.FileType('w'),
		help = "Output location to write data to."
	)

parser.add_argument(
		'--temporary-file',
		type = argparse.FileType('w'),
		default = "tmp.model.pkl",
		help = "Output location to write temporary model to."
	)
parser.add_argument(
		'--max-epochs',
		type = int,
		default = 20,
		help = "Maximum number of epochs to run through training data."
	)
parser.add_argument(
		'--batch-size',
		type = int,
		default = 32,
		help = "Number of sequences to simultaneously calculate cost."
	)
parser.add_argument(
		'--validation-percent',
		type = float,
		default = 0.01,
		help = "First validation_percent of data will be used as the validation set."
	)


parser.add_argument(
		'--checkpoint',
		type = int,
		default = 5000,
		help = "Saves to temporary_file every time this number of instances are seen."
	)

parser.add_argument(
		'--improvement-threshold',
		type = float,
		default = 0.95,
		help = "Improvement over best cost has to be this much or incur patience."
	)
parser.add_argument(
		'--patience',
		type = int,
		default = 10,
		help = "Number of times improvement_threshold is allowed to be crossed since last best cost."
	)


parser.add_argument(
		'--embedding-size',
		type = int,
		default = 100,
		help = "Size of per character embedding / character representation vector."
	)
parser.add_argument(
		'--hidden-size',
		type = int,
		default = 100,
		help = "Size of hidden LSTM layers."
	)
parser.add_argument(
		'--l2',
		type = float,
		default = 0.0,
		help = "L2 coefficient."
	)

args = parser.parse_args()

import sys
import cPickle as pickle
from itertools import islice
from pprint import pprint

import numpy as np
import theano
import theano.tensor as T

from theano_toolkit.parameters import Parameters
from theano_toolkit import updates

import vocab
import model
import data_io

import itertools
def make_batch_train(P,cost,end_id):
	batch = T.imatrix('batch')
	costs,disp_costs = cost(batch,P)
	batch_cost = T.mean(costs)

	print "Calculating gradient..."
	params = P.values()
	grads = T.grad(batch_cost,wrt=params)
	grads_norms = [ T.sqrt(T.sum(g**2)) for g in grads ]
	deltas      = [ T.switch(T.gt(n,5),5*g/n,g)
					for n,g in zip(grads_norms,grads) ]
	


	print "Compiling function..."
	_train = theano.function(
			inputs = [batch],
			outputs = T.mean(disp_costs),
			updates = updates.rmsprop(params,deltas)
		)
	def train(batch):
		max_length = max(len(l) for l in batch)
		batch_array = end_id * np.ones((len(batch),max_length),dtype=np.int32)
		for i,l in enumerate(batch):
			batch_array[i,:len(l)] = l
		return _train(batch_array)
	print "Done."
	return train

def make_test(P,cost):
	X = T.imatrix('X')
	_test = theano.function(
			inputs = [X],
			outputs = cost(X,P)[1],
		)
	def test(test_stream):
		total_cost = 0
		total_count = 0
		for seq in test_stream:
			total_cost  += _test(np.array(seq,dtype=np.int32).reshape(1,len(seq)))
			total_count += 1
		return total_cost/total_count
	return test

if __name__ == "__main__":
	data_file = args.data_file
	vocab_file  = args.vocab_file
	output_file = args.output_file.name
	tmp_file    = args.temporary_file.name
	args.output_file.close()
	args.temporary_file.close()

	filename = data_file.name
	vocab_file_name = vocab_file.name

	max_epochs = args.max_epochs
	batch_size = args.batch_size
	improvement_threshold = args.improvement_threshold
	validation_percent = args.validation_percent
	patience = args.patience
	checkpoint = args.checkpoint

	embedding_size = args.embedding_size
	hidden_size = args.hidden_size

	l2_coefficient = args.l2

	id2char = pickle.load(vocab_file)
	char2id = vocab.load(vocab_file_name)
	P = Parameters()
	lang_model = model.build(P,
			character_count = len(char2id) + 1,
			embedding_size = embedding_size,
			hidden_size = hidden_size
		)

	def cost(X,P): # batch_size x time
		eps = 1e-3
		X = X.T 										# time x batch_size
		char_prob_dist = lang_model(X[:-1])				# time x batch_size x output_size
		char_prob_dist = (1 - 2 * eps) * char_prob_dist + eps
		label_prob = char_prob_dist[
				T.arange(X.shape[0]-1).dimshuffle(0,'x'),
				T.arange(X.shape[1]).dimshuffle('x',0),
				X[1:]
			]												# time x batch_size
		cross_entropy = -T.sum(T.log(label_prob),axis=0)
		display_cost = 2**(-T.mean(T.log2(label_prob),axis=0))
		l2 = sum(T.sum(p**2) for p in P.values())
		cost = cross_entropy
		if l2_coefficient > 0:
			cost += l2_coefficient * l2
		return cost, display_cost
		
	params = P.values()
	train = make_batch_train(P,cost,end_id=char2id["\n"])
	test  = make_test(P,cost)


	# Count number of items in the dataset
	line_count = sum(1 for _ in data_file)
	validation_count = int(round(line_count * validation_percent))
	print "Validation set:",validation_count

	# Training process

	best_cost = np.inf
	increase_count = 0
	seen = 0
	for epoch in xrange(max_epochs):
		print "Epoch:",epoch + 1
		print "Batch size:",batch_size

		# Run test on validation set
		data_stream = data_io.stream(filename,char2id)
		test_stream = islice(data_stream,validation_count)
		test_cost = test(test_stream)
		print "Perplexity:",test_cost

		if test_cost < improvement_threshold * best_cost:
			best_cost = test_cost
			P.save(output_file)
			increase_count = 0
		else:
			increase_count += 1
			if increase_count > patience:
				break


		# Run training
		data_stream = data_io.randomise(data_stream,buffer_size=1024)
		data_stream = data_io.sortify(data_stream,key=lambda x:len(x),buffer_size=512)
		batch_data_stream = data_io.batch(data_stream,batch_size=batch_size)
		batch_data_stream = data_io.randomise(batch_data_stream)

		for batch in batch_data_stream:
			avg_cost = train(batch)
			if np.isnan(avg_cost):
				pprint([''.join(id2char[c] for c in l[1:]) for l in batch])
				exit(1)
			print avg_cost
			seen += len(batch)
			if seen > checkpoint:
				print "Saving..."
				P.save(tmp_file)
				seen = 0
	print "Best cost was",best_cost,"model saved at",output_file
